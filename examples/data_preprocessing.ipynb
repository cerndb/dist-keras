{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "**Joeri Hermans** (Technical Student, IT-DB-SAS, CERN)             \n",
    "*Departement of Knowledge Engineering*         \n",
    "*Maastricht University, The Netherlands*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 October 2016\r\n"
     ]
    }
   ],
   "source": [
    "!(date +%d\\ %B\\ %G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be preprocessing a **4.6 GB** CSV file and saving the results to a Parquet format for further analysis. We will not include the preprocessing step in the analysis because this will take too much time. After the completion of this notebook, we will have a preprocessed dataset ready for training and testing.\n",
    "\n",
    "## Spark preparation and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from distkeras.distributed import *\n",
    "from distkeras.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modify these variables according to your needs.\n",
    "application_name = \"Distributed Deep Learning: Data Prerocessing\"\n",
    "using_spark_2 = False\n",
    "yarn = \"p01001532067275.cern.ch:8088\" # Address:port of ResourceManager\n",
    "if not yarn:\n",
    "    # Tell master to use local resources.\n",
    "    master = \"local[*]\"\n",
    "    num_cores = 3\n",
    "    num_executors = 1\n",
    "else:\n",
    "    # Tell master to use YARN.\n",
    "    master = \"yarn-client\"\n",
    "    num_executors = 4\n",
    "    num_cores = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Use the DataBricks CSV reader, this has some nice functionality regarding invalid values.\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", application_name)\n",
    "conf.set(\"spark.master\", master)\n",
    "conf.set(\"spark.executor.cores\", `num_cores`)\n",
    "conf.set(\"spark.executor.instances\", `num_executors`)\n",
    "conf.set(\"spark.executor.memory\",\"2g\")\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n",
    "\n",
    "# Check if the user is running Spark 2.0 +\n",
    "if using_spark_2:\n",
    "    sc = SparkSession.builder.config(conf=conf) \\\n",
    "            .appName(application_name) \\\n",
    "            .getOrCreate()\n",
    "else:\n",
    "    # Create the Spark context.\n",
    "    sc = SparkContext(conf=conf)\n",
    "    # Add the missing imports\n",
    "    from pyspark import SQLContext\n",
    "    sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocessing\n",
    "\n",
    "### Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if we are using Spark 2.0\n",
    "if using_spark_2:\n",
    "    reader = sc\n",
    "else:\n",
    "    reader = sqlContext\n",
    "# Read the dataset.\n",
    "raw_dataset = reader.read.format('com.databricks.spark.csv') \\\n",
    "                    .options(header='true', inferSchema='true').load(\"data/atlas_higgs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EventId: integer (nullable = true)\n",
      " |-- DER_mass_MMC: double (nullable = true)\n",
      " |-- DER_mass_transverse_met_lep: double (nullable = true)\n",
      " |-- DER_mass_vis: double (nullable = true)\n",
      " |-- DER_pt_h: double (nullable = true)\n",
      " |-- DER_deltaeta_jet_jet: double (nullable = true)\n",
      " |-- DER_mass_jet_jet: double (nullable = true)\n",
      " |-- DER_prodeta_jet_jet: double (nullable = true)\n",
      " |-- DER_deltar_tau_lep: double (nullable = true)\n",
      " |-- DER_pt_tot: double (nullable = true)\n",
      " |-- DER_sum_pt: double (nullable = true)\n",
      " |-- DER_pt_ratio_lep_tau: double (nullable = true)\n",
      " |-- DER_met_phi_centrality: double (nullable = true)\n",
      " |-- DER_lep_eta_centrality: double (nullable = true)\n",
      " |-- PRI_tau_pt: double (nullable = true)\n",
      " |-- PRI_tau_eta: double (nullable = true)\n",
      " |-- PRI_tau_phi: double (nullable = true)\n",
      " |-- PRI_lep_pt: double (nullable = true)\n",
      " |-- PRI_lep_eta: double (nullable = true)\n",
      " |-- PRI_lep_phi: double (nullable = true)\n",
      " |-- PRI_met: double (nullable = true)\n",
      " |-- PRI_met_phi: double (nullable = true)\n",
      " |-- PRI_met_sumet: double (nullable = true)\n",
      " |-- PRI_jet_num: integer (nullable = true)\n",
      " |-- PRI_jet_leading_pt: double (nullable = true)\n",
      " |-- PRI_jet_leading_eta: double (nullable = true)\n",
      " |-- PRI_jet_leading_phi: double (nullable = true)\n",
      " |-- PRI_jet_subleading_pt: double (nullable = true)\n",
      " |-- PRI_jet_subleading_eta: double (nullable = true)\n",
      " |-- PRI_jet_subleading_phi: double (nullable = true)\n",
      " |-- PRI_jet_all_pt: double (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Double-check the inferred schema, and get fetch a row to show how the dataset looks like.\n",
    "raw_dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature processing\n",
    "\n",
    "Next, we will take all the columns in the CSV except the *EventId*, *Weight*, and *Label* column since they are not relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([138.47, 51.655, 97.827, 27.98, 0.91, 124.711, 2.666, 3.064, 41.928, 197.76, 1.582, 1.396, 0.2, 32.638, 1.017, 0.381, 51.626, 2.273, -2.414, 16.824, -0.277, 258.733, 2.0, 67.435, 2.15, 0.444, 46.062, 1.24, -2.475, 113.497]))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we would like to extract the desired features from the raw dataset.\n",
    "# We do this by constructing a list with all desired columns.\n",
    "features = raw_dataset.columns\n",
    "features.remove('EventId')\n",
    "features.remove('Weight')\n",
    "features.remove('Label')\n",
    "# Next, we use Spark's VectorAssembler to \"assemble\" (create) a vector of all desired features.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#vectorassembler\n",
    "vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "# This transformer will take all columns specified in features, and create an additional column \"features\" which will contain all the desired features aggregated into a single vector.\n",
    "dataset = vector_assembler.transform(raw_dataset)\n",
    "\n",
    "# Show what happened after applying the vector assembler.\n",
    "# Note: \"features\" column got appended to the end.\n",
    "dataset.select(\"features\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply feature normalization with standard scaling. This will transform a feature to have mean 0, and std 1.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#standardscaler\n",
    "standard_scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_normalized\", withStd=True, withMean=True)\n",
    "standard_scaler_model = standard_scaler.fit(dataset)\n",
    "dataset = standard_scaler_model.transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Label=u's', label_index=1.0),\n",
       " Row(Label=u'b', label_index=0.0),\n",
       " Row(Label=u'b', label_index=0.0),\n",
       " Row(Label=u'b', label_index=0.0),\n",
       " Row(Label=u'b', label_index=0.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we look at the dataset, the Label column consists of 2 entries, i.e., b (background), and s (signal).\n",
    "# Our neural network will not be able to handle these characters, so instead, we convert it to an index so we can indicate that output neuron with index 0 is background, and 1 is signal.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#stringindexer\n",
    "label_indexer = StringIndexer(inputCol=\"Label\", outputCol=\"label_index\").fit(dataset)\n",
    "dataset = label_indexer.transform(dataset)\n",
    "\n",
    "# Show the result of the label transformation.\n",
    "dataset.select(\"Label\", \"label_index\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of classes (signal and background).\n",
    "nb_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label_index=1.0, label=DenseVector([0.0, 1.0]))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We observe that Keras is not able to work with these indexes. What it actually expects is a vector with an identical size to the output layer. Our framework provides functionality to do this with ease. What it basically does, given an expected vector dimension, it prepares zero vector with the specified dimensionality, and will set the neuron with a specific label index to one.\n",
    "\n",
    "# For example:\n",
    "# 1. Assume we have a label index: 3\n",
    "# 2. Output dimensionality: 5\n",
    "# With these parameters, we obtain the following vector in the DataFrame column: [0,0,0,1,0]\n",
    "dataset = dataset.select(\"features_normalized\", \"label_index\")\n",
    "label_vector_transformer = LabelVectorTransformer(output_dim=nb_classes, input_col=\"label_index\", output_col=\"label\")\n",
    "dataset = label_vector_transformer.transform(dataset)\n",
    "# Only select the columns we need (less data shuffling) while training.\n",
    "dataset = dataset.select(\"features_normalized\", \"label_index\", \"label\")\n",
    "\n",
    "# Show the expected output vectors of the neural network.\n",
    "dataset.select(\"label_index\", \"label\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset randomization\n",
    "\n",
    "We shuffle the complete dataset in order to be able to draw stochastic samples from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Randomize the dataset.\n",
    "dataset = shuffle(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset saving\n",
    "\n",
    "Finally, we save the shuffled and processed dataset to disk for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store the preprocessed dataset as a Parquet file.\n",
    "dataset.write.save(\"data/processed.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 1697.053406 seconds.\n",
      "Total time: 28.2842234333 minutes.\n"
     ]
    }
   ],
   "source": [
    "time_end = time.time()\n",
    "dt = time_end - time_start\n",
    "print(\"Total time: \" + str(dt) + \" seconds.\")\n",
    "print(\"Total time: \" + str(dt / 60) + \" minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
